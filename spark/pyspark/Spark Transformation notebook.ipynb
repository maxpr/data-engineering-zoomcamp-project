{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a177e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7c5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_project_id = os.getenv('GOOGLE_PROJECT_ID')\n",
    "google_biquery_dataset = os.getenv('GOOGLE_BIGQUERY_DATASET')\n",
    "google_cloud_storage_id = os.getenv('GOOGLE_CLOUD_STORAGE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a114d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj ID validation-project-de-zoomcamp withcloud storage id at million_song_data_lake_validation-project-de-zoomcamp and bigquery dataset being million_song\n"
     ]
    }
   ],
   "source": [
    "print(f\"proj ID {google_project_id} withcloud storage id at {google_cloud_storage_id} and bigquery dataset being {google_biquery_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5313116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-27 09:57:46--  https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.10.128, 142.251.12.128, 172.217.194.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.10.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31607894 (30M) [application/java-archive]\n",
      "Saving to: ‘/spark/gcs-connector-hadoop3-latest.jar’\n",
      "\n",
      "/spark/gcs-connecto 100%[===================>]  30.14M  18.4MB/s    in 1.6s    \n",
      "\n",
      "2022-03-27 09:57:48 (18.4 MB/s) - ‘/spark/gcs-connector-hadoop3-latest.jar’ saved [31607894/31607894]\n",
      "\n",
      "--2022-03-27 09:57:48--  https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-latest_2.12.jar\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.10.128, 142.251.12.128, 172.217.194.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.10.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 56477120 (54M) [application/java-archive]\n",
      "Saving to: ‘/spark/spark-bigquery-latest_2.12.jar’\n",
      "\n",
      "/spark/spark-bigque 100%[===================>]  53.86M  18.5MB/s    in 2.9s    \n",
      "\n",
      "2022-03-27 09:57:52 (18.5 MB/s) - ‘/spark/spark-bigquery-latest_2.12.jar’ saved [56477120/56477120]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar -O /spark/gcs-connector-hadoop3-latest.jar\n",
    "!wget https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-latest_2.12.jar -O /spark/spark-bigquery-latest_2.12.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2004580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/27 09:57:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"/spark/gcs-connector-hadoop3-latest.jar,/spark/spark-bigquery-latest_2.12.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/.google/google_credentials.json\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"/.google/google_credentials.json\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf792bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e55cacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/27 09:57:59 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://million_song_data_lake_validation-project-de-zoomcamp/raw/millionsongsubset.parquet.\n",
      "java.io.IOException: Error accessing gs://million_song_data_lake_validation-project-de-zoomcamp/raw/millionsongsubset.parquet\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2155)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2043)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n",
      "\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:596)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\n",
      "GET https://storage.googleapis.com/storage/v1/b/million_song_data_lake_validation-project-de-zoomcamp/o/raw%2Fmillionsongsubset.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n",
      "{\n",
      "  \"code\" : 403,\n",
      "  \"errors\" : [ {\n",
      "    \"domain\" : \"global\",\n",
      "    \"message\" : \"test-account@alpine-freedom-344703.iam.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n",
      "    \"reason\" : \"forbidden\"\n",
      "  } ],\n",
      "  \"message\" : \"test-account@alpine-freedom-344703.iam.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n",
      "}\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2149)\n",
      "\t... 24 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o143.parquet.\n: java.io.IOException: Error accessing gs://million_song_data_lake_validation-project-de-zoomcamp/raw/millionsongsubset.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2155)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2043)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:779)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/million_song_data_lake_validation-project-de-zoomcamp/o/raw%2Fmillionsongsubset.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"test-account@alpine-freedom-344703.iam.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n    \"reason\" : \"forbidden\"\n  } ],\n  \"message\" : \"test-account@alpine-freedom-344703.iam.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2149)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_million_song \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgoogle_cloud_storage_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/raw/millionsongsubset.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df_million_song \u001b[38;5;241m=\u001b[39m df_million_song\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      7\u001b[0m df_million_song\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/readwriter.py:301\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    295\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema, pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m    297\u001b[0m                recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup, modifiedBefore\u001b[38;5;241m=\u001b[39mmodifiedBefore,\n\u001b[1;32m    298\u001b[0m                modifiedAfter\u001b[38;5;241m=\u001b[39mmodifiedAfter, datetimeRebaseMode\u001b[38;5;241m=\u001b[39mdatetimeRebaseMode,\n\u001b[1;32m    299\u001b[0m                int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o143.parquet.\n: java.io.IOException: Error accessing gs://million_song_data_lake_validation-project-de-zoomcamp/raw/millionsongsubset.parquet\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2155)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:2043)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1091)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1065)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:955)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:779)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\nCaused by: com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden\nGET https://storage.googleapis.com/storage/v1/b/million_song_data_lake_validation-project-de-zoomcamp/o/raw%2Fmillionsongsubset.parquet?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata\n{\n  \"code\" : 403,\n  \"errors\" : [ {\n    \"domain\" : \"global\",\n    \"message\" : \"test-account@alpine-freedom-344703.iam.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\",\n    \"reason\" : \"forbidden\"\n  } ],\n  \"message\" : \"test-account@alpine-freedom-344703.iam.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object.\"\n}\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:2149)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "df_million_song = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet(f\"gs://{google_cloud_storage_id}/raw/millionsongsubset.parquet\")\n",
    "\n",
    "df_million_song = df_million_song.repartition(4)\n",
    "\n",
    "df_million_song.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52849b5",
   "metadata": {},
   "source": [
    "# What each column represent ?\n",
    "\n",
    "### analysis_sample_rate\n",
    "- The sampling frequency or sampling rate, fs, is the average number of samples obtained in one second, thus fs = 1/T. **Int**\n",
    "\n",
    "### artist_7digitalid\n",
    "- The artist 7digital id -> https://us.7digital.com/artist/and-id **Int*\n",
    "\n",
    "### artist_familiarity\n",
    "- Familiarity is an indication of how well known the artist is. **Float**\n",
    "\n",
    "### artist_hotttnesss\n",
    "- Hotness is an indication of how much buzz the artist is getting  right now **Float**\n",
    "\n",
    "### artist_id\n",
    "- Id of artist in millionsong dataset **String**\n",
    "\n",
    "### artist_latitude \tartist_location \tartist_longitude \t\n",
    "- If present Longitue / Latitude as **Float** and location as **String**\n",
    "\n",
    "### artist_mbid\n",
    "- Id from musicBrainz for the artist **String**\n",
    "\n",
    "###  artist_mbtags     artist_mbtags_count\n",
    "- Array of tags representing the artist, as well as the count of those tags from MusicBrainz **List[String] && List[Int]**\n",
    "\n",
    "### artist_name\n",
    "- Name of artist **String**\n",
    "\n",
    "### artist_playmeid\n",
    "- Playme Id of the artist\n",
    "\n",
    "### artist_terms       artist_terms_freq      artist_terms_weight\n",
    "- Array of tags representing the artist, as well as the frequency and weight of those tags from The Echo Nest **List[String] && List[Float]  List[Float]**\n",
    "\n",
    "### audio_md5\n",
    "- hash code of the audio used for the analysis by The Echo Nest **Sring**\n",
    "\n",
    "### bars_confidence\n",
    "- confidence value (between 0 and 1) associated with each bar by The Echo Nest **List[Float]**\n",
    "\n",
    "### bars_start\n",
    "- start time of each bar according to The Echo Nest, this song has 99 bars **List[Float]**\n",
    "\n",
    "### beats_confidence \n",
    "- confidence value (between 0 and 1) associated with each beat by The Echo Nest **List[Float]**\n",
    "\n",
    "### beats_start\n",
    "- start time of each beat according to The Echo Nest**List[Float]**\n",
    "\n",
    "### danceability \n",
    "- danceability measure of this song according to The Echo Nest (between 0 and 1, 0 => not analyzed) **Float**\n",
    "\n",
    "### duration\n",
    "- Duration of track in seconds **Float**\n",
    "\n",
    "### end_of_fade_in\n",
    "- time of the end of the fade in, at the beginning of the song, according to The Echo Nest **Float**\n",
    "\n",
    "### energy\n",
    "- 0.0 everywhere\n",
    "\n",
    "### key\n",
    "- estimation of the key the song is in by The Echo Nest **Int**\n",
    "\n",
    "### key_confidence\n",
    "- confidence of the key estimation **Float**\n",
    "\n",
    "### loudness\n",
    "- general loudness of the track **Float**\n",
    "\n",
    "### mode\n",
    "- estimation of the mode (major or minor) the song is in by The Echo Nest **Int**\n",
    "\n",
    "### mode_confidence\n",
    "- confidence of the mode estimation **Float**\n",
    "\n",
    "### release\n",
    "- album name from which the track was taken, some songs / tracks can come from many albums, we give only one **String**\n",
    "\n",
    "### release_7digitalid\n",
    "- the ID of the release (album) on the service 7digital.com **String**\n",
    "\n",
    "### sections_confidence\n",
    "- confidence value (between 0 and 1) associated with each section by The Echo Nest **List[Float]**\n",
    "\n",
    "### sections_start\n",
    "- start time of each section according to The Echo Nest **List[Float]**\n",
    "\n",
    "### segments_confidence\n",
    "- confidence value (between 0 and 1) associated with each segment by The Echo Nest **List[Float]**\n",
    "\n",
    "### segments_loudness_max\n",
    "- max loudness during each segment **List[Float]**\n",
    "\n",
    "### segments_loudness_max_time\n",
    "-  time of the max loudness during each segment **List[Float]**\n",
    " \n",
    "### segments_loudness_start\n",
    "- loudness at the beginning of each segment **List[Float]**\n",
    " \n",
    "### segments_start\n",
    "- start time of each segment (~ musical event, or onset) according to The Echo Nest **List[Float]**\n",
    " \n",
    "### similar_artists\n",
    "- a list of 100 artists (their Echo Nest ID) similar to Rick Astley according to The Echo Nest **List[String]**\n",
    "\n",
    "### song_hotttnesss\n",
    "- according to The Echo Nest, when downloaded (in December 2010), this song had a 'hotttnesss' of this value (on a scale of 0 and 1) **Float**\n",
    "\n",
    "### song_id\n",
    "-  The Echo Nest song ID, note that a song can be associated with many tracks (with very slight audio differences) **String**\n",
    "\n",
    "### start_of_fade_out\n",
    "-  start time of the fade out, in seconds, at the end of the song, according to The Echo Nest **Float**\n",
    "\n",
    "### tatums_confidence\n",
    "-  confidence value (between 0 and 1) associated with each tatum by The Echo Nest **List[Float]**\n",
    "\n",
    "### tatums_start\n",
    "- start time of each tatum according to The Echo Nest **List[Float]**\n",
    "\n",
    "### tempo\n",
    "- Tempo of the song **Float**\n",
    "\n",
    "### time_signature\n",
    "- time signature of the song according to The Echo Nest, i.e. usual number of beats per bar **Int**\n",
    "\n",
    "### time_signature_confidence\n",
    "- confidence of the time signature estimation **Float**\n",
    "\n",
    "### title\n",
    "- Title of Song **String**\n",
    "\n",
    "### track_7digitalid\n",
    "- the ID of this song on the service 7digital.com **String**\n",
    "\n",
    "### track_id\n",
    "- The Echo Nest ID of this particular track on which the analysis was done **String**\n",
    "\n",
    "### year\n",
    "- year when this song was released, according to musicbrainz.org **Int**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45ac48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's look at the describe\n",
    "df_million_song.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ab671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_million_song.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5609f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Decision for storage efficiency\n",
    "\n",
    "## We will parition by year, even if more than half data is at year 0 (5320)\n",
    "df_million_song.groupBy('year').count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_million_song.groupBy('key').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d27788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_million_song.groupBy('mode').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5239eec",
   "metadata": {},
   "source": [
    "## We transform the tempo to int to be able to leverage cluster of bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca3bf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df_million_song = df_million_song.withColumn('tempo_int', (df_million_song.tempo).cast(IntegerType())).drop('tempo')\n",
    "df_million_song.groupBy('tempo_int').count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = df_million_song.drop('bars_confidence','bars_start','beats_confidence', 'beats_start', 'sections_confidence', 'sections_start', 'segments_confidence', 'segments_loudness_max', 'segments_loudness_max_time', 'segments_loudness_start', 'segments_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1605758",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = df_to_save.sample(False, 0.1, seed=0).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb1af45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "record.write \\\n",
    "        .format(\"bigquery\") \\\n",
    "        .option(\"temporaryGcsBucket\",google_cloud_storage_id) \\\n",
    "        .option('table', f\"{google_biquery_dataset}.data\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3295bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SchemaField('analysis_sample_rate', 'INTEGER', 'NULLABLE', None, (), None), SchemaField('artist_7digitalid', 'INTEGER', 'NULLABLE', None, (), None), SchemaField('artist_familiarity', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('artist_hotttnesss', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('artist_id', 'STRING', 'NULLABLE', None, (), None), SchemaField('artist_latitude', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('artist_location', 'STRING', 'NULLABLE', None, (), None), SchemaField('artist_longitude', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('artist_mbid', 'STRING', 'NULLABLE', None, (), None), SchemaField('artist_mbtags', 'RECORD', 'NULLABLE', None, (SchemaField('list', 'RECORD', 'REPEATED', None, (SchemaField('element', 'STRING', 'NULLABLE', None, (), None),), None),), None), SchemaField('artist_mbtags_count', 'RECORD', 'NULLABLE', None, (SchemaField('list', 'RECORD', 'REPEATED', None, (SchemaField('element', 'INTEGER', 'NULLABLE', None, (), None),), None),), None), SchemaField('artist_name', 'STRING', 'NULLABLE', None, (), None), SchemaField('artist_playmeid', 'INTEGER', 'NULLABLE', None, (), None), SchemaField('artist_terms', 'RECORD', 'NULLABLE', None, (SchemaField('list', 'RECORD', 'REPEATED', None, (SchemaField('element', 'STRING', 'NULLABLE', None, (), None),), None),), None), SchemaField('artist_terms_freq', 'RECORD', 'NULLABLE', None, (SchemaField('list', 'RECORD', 'REPEATED', None, (SchemaField('element', 'FLOAT', 'NULLABLE', None, (), None),), None),), None), SchemaField('artist_terms_weight', 'RECORD', 'NULLABLE', None, (SchemaField('list', 'RECORD', 'REPEATED', None, (SchemaField('element', 'FLOAT', 'NULLABLE', None, (), None),), None),), None), SchemaField('audio_md5', 'STRING', 'NULLABLE', None, (), None), SchemaField('danceability', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('duration', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('end_of_fade_in', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('energy', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('key', 'INTEGER', 'NULLABLE', None, (), None), SchemaField('key_confidence', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('loudness', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('mode', 'INTEGER', 'NULLABLE', None, (), None), SchemaField('mode_confidence', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('release', 'STRING', 'NULLABLE', None, (), None), SchemaField('release_7digitalid', 'INTEGER', 'NULLABLE', None, (), None), SchemaField('similar_artists', 'RECORD', 'NULLABLE', None, (SchemaField('list', 'RECORD', 'REPEATED', None, (SchemaField('element', 'STRING', 'NULLABLE', None, (), None),), None),), None), SchemaField('song_hotttnesss', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('song_id', 'STRING', 'NULLABLE', None, (), None), SchemaField('start_of_fade_out', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('tatums_confidence', 'RECORD', 'NULLABLE', None, (SchemaField('list', 'RECORD', 'REPEATED', None, (SchemaField('element', 'FLOAT', 'NULLABLE', None, (), None),), None),), None), SchemaField('tatums_start', 'RECORD', 'NULLABLE', None, (SchemaField('list', 'RECORD', 'REPEATED', None, (SchemaField('element', 'FLOAT', 'NULLABLE', None, (), None),), None),), None), SchemaField('time_signature', 'INTEGER', 'NULLABLE', None, (), None), SchemaField('time_signature_confidence', 'FLOAT', 'NULLABLE', None, (), None), SchemaField('title', 'STRING', 'NULLABLE', None, (), None), SchemaField('track_7digitalid', 'INTEGER', 'NULLABLE', None, (), None), SchemaField('track_id', 'STRING', 'NULLABLE', None, (), None), SchemaField('year', 'INTEGER', 'NULLABLE', None, (), None), SchemaField('tempo_int', 'INTEGER', 'NULLABLE', None, (), None)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "dataset_ref = client.dataset(google_biquery_dataset)\n",
    "table_ref = dataset_ref.table(\"data\")\n",
    "table = client.get_table(table_ref)  # API Request\n",
    "\n",
    "# View table properties\n",
    "schema = table.schema\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85666468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table alpine-freedom-344703.million_song.data_partitioned\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "table_id = f\"{google_project_id}.{google_biquery_dataset}.data_partitioned\"\n",
    "\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table.range_partitioning = bigquery.RangePartitioning(\n",
    "    # To use integer range partitioning, select a top-level REQUIRED /\n",
    "    # NULLABLE column with INTEGER / INT64 data type.\n",
    "    field=\"year\",\n",
    "    range_=bigquery.PartitionRange(start=0, end=2200, interval=10),\n",
    ")\n",
    "\n",
    "table.clustering_fields = [\"tempo_int\", \"mode\", \"key\"]\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the destination table.\n",
    "# table_id = \"your-project.your_dataset.your_table_name\"\n",
    "\n",
    "\n",
    "table = client.create_table(table)  # Make an API request.\n",
    "print(\n",
    "    \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "453d248d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_to_save.write \\\n",
    "        .mode('append') \\\n",
    "        .format(\"bigquery\") \\\n",
    "        .option(\"temporaryGcsBucket\",google_cloud_storage_id) \\\n",
    "        .option('table', f\"{google_biquery_dataset}.data_partitioned\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b3c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
